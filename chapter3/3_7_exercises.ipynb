{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 1\n",
    "Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n",
    "\n",
    "|                | Coefficient | Std. Error | t-statistic | p-value  |\n",
    "|----------------|-------------|------------|-------------|----------|\n",
    "| **Intercept**  | 2.939       | 0.3119     | 9.42        | < 0.0001 |\n",
    "| **TV**         | 0.046       | 0.0014     | 32.81       | < 0.0001 |\n",
    "| **Radio**      | 0.189       | 0.0086     | 21.89       | < 0.0001 |\n",
    "| **Newspaper**  | -0.001      | 0.0059     | -0.18       | 0.8599   |\n",
    "\n",
    "**TABLE 3.4.** *For the Advertising data, least squares coefficient estimates of the multiple linear regression of number of units sold on TV, radio, and newspaper advertising budgets.*\n",
    "\n",
    "Based on the p-values, we can conclude that tv and radio both have a strong association with the number of units sold (due to the p-value close to 0), while newspaper advertising budget is not correlated with units sold. \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "*unanswered*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Suppose we have a data set with five predictors, $X_1 = \\text{GPA}$, $X_2 = \\text{IQ}$, $X_3 = \\text{Level (1 for College and 0 for High School)}$, $X_4 = \\text{Interaction between GPA and IQ}$, and $X_5 = \\text{Interaction between GPA and Level}$. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\hat{\\beta}_0 = 50$, $\\hat{\\beta}_1 = 20$, $\\hat{\\beta}_2 = 0.07$, $\\hat{\\beta}_3 = 35$, $\\hat{\\beta}_4 = 0.01$, $\\hat{\\beta}_5 = -10$.\n",
    "\n",
    "#### Question 3a\n",
    "(a) Which answer is correct, and why?\n",
    "\n",
    "- i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "- ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "- iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "- iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n",
    "\n",
    "Option iv is correct. Examining the estimated model coefficients, we see that $\\hat{\\beta}_3 = 35$ meaning when all other terms are held constant, a college student will earn \\$35,000 more than a high school student. The $\\hat{\\beta}_5 = -10$ means that the effect of GPA on salary when a student is in college vs high school is about -10,000, meaning that for college students, the effect of one increase point \n",
    "\n",
    "#### Question 3b\n",
    "Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n",
    "\n",
    "$(20 * 4.0) + (110 * 0.07) + (35 * 1) + (4.0 * 1 * 0.01) + (4.0 * 1 * -10) = 82.74 $\n",
    "\n",
    "#### Question 3c\n",
    "True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 4\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Since the cubic regression is more flexible, it will more closely match the training data when compared to the linear regression.\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Since the true relationship is linear, the linear regression will likely better fit the new test data points. The cubic regression, due to its flexibility, would have overfit the training data leading to higher residuals for the test data. \n",
    "\n",
    "#### Question 4c\n",
    "\n",
    "For the same reason as in part a, the cubic regression's flexibility will lead it to better fit the training data compared to the linear regression. \n",
    "\n",
    "#### Question 4d\n",
    "\n",
    "For the test data, if the real world relationship is cubic, there is the potential for the cubic regression to more closely model the data. However, since the cubic regression is also more prone to overfitting, depending on the real world relationships, the linear regression may have lower residuals. \n",
    "\n",
    "\n",
    "**Takeaways:**\n",
    "- the higher order polynomial terms in cubic (and higher) regressions lead to more flexibility (and ability to hit more training data points)\n",
    "  - when a model's complexity is higher than the real world relationship, it leads to overfitting as it starts to tune to the noise in the training data\n",
    "  - usually means that higher order polynomials will better fit the training data, but the overfitting will lead to worse residuals in the test data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "*unanswered*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Using the $\\hat{\\beta}_0$ minimizer ($\\hat{\\beta}_0 = \\overline{y}-\\hat{\\beta}_1\\hat{x})$, we can substitute $\\overline{x}$ for $x$ in the linear regression equation $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 * \\hat{x} \\\\\n",
    "        &= (\\overline{y} - \\hat{\\beta}_1\\hat{x}) + \\hat{\\beta}_1\\hat{x} \\\\\n",
    "        &= \\overline{y} - \\hat{\\beta}_1\\hat{x} + \\hat{\\beta}_1\\hat{x} \\\\\n",
    "\\hat{y} &= \\overline{y} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, when $x =\\overline{x}$, $y=\\overline{y}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
